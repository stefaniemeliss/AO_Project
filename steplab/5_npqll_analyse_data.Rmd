---
title: "NPQLL - Analysis"
author: "Stefanie Meliss"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
options(scipen = 999)
# empty work space
rm(list = ls())

# define directory
dir_step <- getwd()

# load libraries
library(tidyr)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(mirt)

# load in functions
devtools::source_url("https://github.com/stefaniemeliss/AO_Project/blob/main/pilots/functions.R?raw=TRUE")

# function to determine outliers
is_outlier_iqr <- function(x) {
  # +/- 1.5*IQR
  return(x < quantile(x, 0.25, na.rm = T) - 1.5 * IQR(x, na.rm = T) | x > quantile(x, 0.75, na.rm = T) + 1.5 * IQR(x, na.rm = T))
}


```



```{r, include=FALSE}
# --- Read in data --- #

# read in file that was past to LMS to determine grouptag
rand <- read.csv(file = file.path(dir_step, "npqll_random_group_assignment.csv"))

# read in data
pre <- read.csv(file = file.path(dir_step, "processed_data_course_2_module_1.csv"))
post <- read.csv(file = file.path(dir_step, "processed_data_course_4_module_1.csv"))
dt <- read.csv(file = file.path(dir_step, "processed_data_timestamps.csv"))

# merge ALL data
all <- merge(rand, pre, by = "user_id", all = T)
all <- merge(all, post, by = "user_id", all = T)
all <- merge(all, dt, by = "user_id", all = T)

# remove participants who provided non-responses in posttest free text items
all <- all %>% 
  filter(! user_id %in% c("US0czjy1n8wg", "USujo2jn-fwi"))

# merge complete data
df <- all[!is.na(all$score_posttest), ]

```

### Sample size

```{r, echo=FALSE, results='asis'}


cat("Data collection for the project closed on October 31st 2023 at 23:59. Up to this time point, we received N = ", nrow(pre), "responses to the prior knowledge test and N = ", nrow(post), "responses to the learning outcome assessment. ")
cat("There is complete data (i.e., pre- and post-test measures) from N = ", sum(post$user_id %in% pre$user_id), "participants. ")


cat("Upon careful inspection of the learning outcome assessment data, two participants were identified who provided non-responses in at least 14 out of 15 free-text items. Data from these two participants were excluded from the analyses. \n\n")

```

> Data from other participants may be excluded upon receiving the engagement data.


### Manipulation check  

To determine whether any participants in that were exposed to the AO (non-AO) were indeed part of the experimental (control) group, we compared the user ids of participants randomly *assigned* to each group against the user ids of participants *exposed* to the respective introductory material.  


```{r, echo=FALSE, results='asis'}
# --- Manipulation check --- #

# extract user_id as vector for each group
exp <- rand$user_id[rand$group == "AO"]
con <- rand$user_id[rand$group == "non-AO"]

# get user_ids of ppt that have accessed AO or non-AO
AO <- dt$user_id[!is.na(dt$dt_a_f_mat_intr_exp)]
nAO <- dt$user_id[!is.na(dt$dt_a_f_mat_intr_cont)]

# check whether any user has accessed AO (non-AO) that was NOT part of the experimental (control) group
if (sum(! AO %in% exp) == 0) {
  cat("No participant has accessed the AO that was not part of the experimental group. ")
} else { cat("Some participants that were not part of the experimental group were exposed to the AO.")}


if (sum(! nAO %in% con) == 0) {
  cat("No participant has accessed the non-AO that was not part of the control group. ")
} else { cat("Some participants that were not part of the control group were exposed to the non-AO.")}

cat("\n")

# check how many participants in the experimental (control) group did not access the AO (non-AO)
if (sum(is.na(df$dt_a_f_mat_intr_exp[df$group == "AO"])) == 0) {
  cat("All participant in the experimental group have accessed the AO. ")
} else { cat("Some participants (N = ", sum(is.na(df$dt_a_f_mat_intr_exp[df$group == "AO"])), ") in the experimental group have not accessed the AO. ", sep = "")}

if (sum(is.na(df$dt_a_f_mat_intr_cont[df$group == "non-AO"])) == 0) {
  cat("All participant in the experimental group have accessed the AO. ")
} else { cat("Some participants (N = ", sum(is.na(df$dt_a_f_mat_intr_cont[df$group == "non-AO"])), ") in the control group have not accessed the non-AO. ", sep = "")}

cat("\n")

# code variable that captures whether introductory material was accessed or not
df$access_mat_intro <- ifelse(df$group == "AO" & !is.na(df$dt_a_f_mat_intr_exp), TRUE,
                              ifelse(df$group == "non-AO" & !is.na(df$dt_a_f_mat_intr_cont), TRUE,
                                     FALSE))

# remove participants that have not accessed the assigned introductory materials
if (sum(is.na(df$dt_a_f_mat_intr_exp[df$group == "AO"])) != 0 | sum(is.na(df$dt_a_f_mat_intr_cont[df$group == "non-AO"])) != 0) {
  df <- df[df$access_mat_intro, ]

  cat("The respective participants were removed from further data analysis. As such, the sample analysed here consisted of N =", nrow(df), "participants.")
} 

cat("\n")

```

### Descriptive characteristics

##### Prior knowledge assessment

To assess participants’ prior knowledge on the topic of the learning material (i.e., development of reading abilities), a 10-item four-alternative forced-choice test was developed. The scope of the prior knowledge assessment was for each item to target a different construct relevant to the development of reading abilities (e.g., “An orthography is…”). For each item, the correct answer (i.e., “The set of conventions associated with a written language.”) together with three plausible distractors (e.g., “The set of conventions associated with a phonics system.”) was presented. 

Participants were instructed to select the answer option that they think is correct and were encouraged to guess but discouraged from looking up the correct responses. The order of presentation of the ten items and corresponding distractors was randomised, and the same random order was applied across all participants. All items were presented on a single screen. Responses could only be submitted once each of the ten items has been completed. 

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# define vars
var <- "score_pretest"
grp <- "group"

# generate and print descriptive tables to markdown
table_desc(data = df, group_var = grp, dep_var = var)    

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(data = df, xvar = grp, yvar = var, 
                      xlower = 1, xupper = 2,
                      ylower = 0, yupper = 10, ybreaks = 1,
                      #title = "Descriptive visualisation of rating in each group",
                      xlab = "Introductory material",
                      ylab = "Total number of correct items", 
                      note = "Error bars represent SE.")

# run two sample t-test
tmp <- t.test(df$experience ~ df$group)

# extract values
t_val <- tmp$statistic
p_val <- tmp$p.value
df_val <- tmp$parameter

# add to markdown
if (p_val > 0.05) {
  cat("No significant difference in prior knowledge was found between the groups, t(", round(df_val, 2), ") = ", round(t_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
} else {
  cat("A significant difference in prior knowledge was found between the groups, t(", round(df_val, 2), ") = ", round(t_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
}


```


##### Learning outcome assessment

To determine whether exposure to an AO facilitates learning, an expert in the field (CS) developed a learning outcome assessment for the study. The learning outcome assessment targets information presented in the learning material. We created a set of 23 items capturing knowledge retention and transfer. The resulting learning outcome assessment consisted of 15 short answer items (e.g., “Complete this sentence: The smallest chunk of meaning within a word is called a __________.”) and 8 four-alternative forced-choice items. For each presented item (e.g., “Which of these are examples of comprehension strategies that might be used by pupils to support them to build meaning from a challenging text?”), participants were instructed to select all correct answers out of four alternatives they thought as correct (e.g., “Clarifying”, “Summarising”, “Highlighting”, “Blending”). 

For each of the short answer items (e.g., “Please complete the following sentence: With extensive practice, the process of word recognition develops a sense of flow. This flow of words is called __________.”), the correct solution was determined before the assessment was administered (e.g., “(reading) fluency”).  Additionally, all given answers were reviewed by a rater (CS) blind to the experimental condition and alternative acceptable solutions (e.g., “prosody”, “automaticity”) were identified. For each multiple-choice item with its four answer options, the pattern of selected/unselected options was compared against the correct pattern. This means that a point was awarded if an option was correctly selected and if an option was correctly not selected. However, due to an error, one multiple-choice item was set up to be a single-choice item, i.e., only one answer option could be selected. This item was scored as if it was a single item and a point was awarded if the correct answer was selected, but not if the incorrect answers were not selected. The maximum total score on the multiple-choice items was therefore 29. 

Participants were provided with matching instructions for each item type. Participants were informed that incorrect answers in the multiple-choice items will be penalised. Further, they were instructed to not look up responses. The order of presentation of the items (and corresponding distractors, in case of multiple-choice items) was randomised, and the same random order was be applied across all participants. All items will be presented on a single screen. Responses could only be submitted if each of the items has been completed. 

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# define vars
var <- "score_posttest"
grp <- "group"

# generate and print descriptive tables to markdown
table_desc(data = df, group_var = grp, dep_var = var)    

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(data = df, xvar = grp, yvar = var, 
                      xlower = 1, xupper = 2,
                      #ylower = 15, yupper = 45, ybreaks = 1,
                      xlab = "Introductory material",
                      ylab = "Total number of points", 
                      note = "Error bars represent SE.")

```

##### Change in knowledge

To determine whether there was an overall change in knowledge, BLA BLA

```{r, echo = F, fig.align='center', warning=FALSE}
# standardise scores
df$score_pretest_c <- scale(df$score_pretest)
df$score_posttest_c <- scale(df$score_posttest)

# convert data into long format
df_l <- df %>% 
  select(user_id, group, score_pretest_c, score_posttest_c) %>%
  pivot_longer(cols = c("score_pretest_c", "score_posttest_c"),
               names_to = "timepoint", 
               values_to = "score_c")

df_l$timepoint <- ifelse(df_l$timepoint == "score_pretest_c", "Prior knowledge", "Learning outcome")
df_l$timepoint <- factor(df_l$timepoint, levels = c("Prior knowledge", "Learning outcome"))

# create a spaghetti plot
plt <- ggplot(data = df_l, aes(x = timepoint, y = score_c, group = user_id)) +
  geom_line(color = nondominant_col) +
  facet_wrap(. ~ group, nrow = 2) +
  stat_summary(fun="mean", geom = "point", col = dominant_col, group = 1) + 
  stat_summary(fun.data = mean_se, geom = "errorbar", width = .05, col = dominant_col, group = 1) +
  xlab("Assessment") + ylab("Standardised score") + labs(col = "Introductory material") +
  labs(caption = "Error bars represent SE.") +
  theme +
  theme(plot.caption = element_text(hjust=0))
plt

```


### Primary hypothesis test

Our primary hypothesis states that the exposure to an AO will improve the encoding of later presented, to-be-learned material. To test this hypothesis, an ANCOVA is used. We are interested in the main effect of experimental condition, i.e., whether an AO or historical overview, presented as introductory material, affects learning of the evidence summary (measured as the sum score of the learning outcome assessment), after controlling for prior knowledge at baseline (measured as the sum score of the prior knowledge assessment). The sum scores were standardised and the independent variable was dummy-coded (0 = "non-AO", 1 = "AO").  

```{r, echo = F, fig.align='center', warning=FALSE}

# effect code group
df$group_e <- ifelse(df$group == "AO", 1, 
                     ifelse(df$group == "non-AO", -1, NA))
# dummy code group
df$group_d <- ifelse(df$group == "AO", 1, 
                     ifelse(df$group == "non-AO", 0, NA))

```

##### Assumption checks

Before conducting the ANCOVA, assumptions were checked. A Shapiro-Wilk test was used to determine whether residuals in the model were normally distributed.
```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# determine the ANCOVA model
m1 <- lm(score_posttest_c ~ group_d + score_pretest_c, data = df)

# check whether residuals are normally distributed
tmp <- shapiro.test(m1$residuals)

# extract values
p_val <- tmp$p.value
stat <- tmp$statistic

# add to markdown
if (p_val > 0.05) {
  cat("The results of the Shapiro-Wilk test indicated that the residuals did not significantly deviate from normality, W = ", round(stat, 3), ", p = ", round(p_val, 3), ". This is further shown in the model plots below.", sep = "")
} else {
  cat("The results of the Shapiro-Wilk test indicated that the residuals significantly deviated from normality, W = ", round(stat, 3), ", p = ", round(p_val, 3), ". This is further shown in the model plots below.", sep = "")
}

# plot model
par(mfrow = c(2, 2))
plot(m1)

```

Due to the assumption violation, participants with outlying values on the dependent variable were identified. Outliers were defined as observations that are more than 1.5 IQR below Q1 or more than 1.5 IQR above Q3.
```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# identify outliers: 1.5 * IQR
df$outlier_iqr_pretest <- is_outlier_iqr(df$score_pretest)
df$outlier_iqr_posttest <- is_outlier_iqr(df$score_posttest)
df$outlier_iqr <- ifelse(is.na(df$outlier_iqr_pretest) | is.na(df$outlier_iqr_posttest), NA,
                         ifelse(df$outlier_iqr_pretest| df$outlier_iqr_posttest, T, F))

# add to markdown
cat("As such, ", sum(df$outlier_iqr_posttest), "outliers were identified and removed and the Shapiro-Wilk test was repeated.")

# remove outliers
df <- df[df$outlier_iqr_posttest == F, ]
```



```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# run ANOCOVA without outliers IQR
m2 <- lm(score_posttest_c ~ group_d + score_pretest_c, data = df)

# check whether residuals are normally distributed
tmp <- shapiro.test(m2$residuals)

# extract values
p_val <- tmp$p.value
stat <- tmp$statistic

# add to markdown
if (p_val > 0.05) {
  cat("The results of the Shapiro-Wilk test indicated that, following the removal of outliers, the residuals did not significantly deviate from normality, W = ", round(stat, 3), ", p = ", round(p_val, 3), ". This is further shown in the model plots below.", sep = "")
} else {
  cat("The results of the Shapiro-Wilk test indicated that, following the removal of outliers, the residuals significantly deviated from normality, W = ", round(stat, 3), ", p = ", round(p_val, 3), ". This is further shown in the model plots below.", sep = "")
}

# plot model
par(mfrow = c(2, 2))
plot(m2)
```


To determine linearity between the covariate and the outcome variable at each level of the independent variable (i.e., introductory material manipulation), a grouped scatter plot was created. The scatter plot shows the covariate (i.e., score in the prior knowledge assessment) on the x-axis and the dependent variable (i.e., score in the learning outcome assessment) on the y-axis. Data is shown in separate panels, depending on the kind of introductory material presented. Regression slopes predicting the dependent variable using the covariate were added to check the assumption of homogeneity of regression slopes. As shown below, the regression lines are parallel.
```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# assumption of homogeneity of regression slopes
# check whether interaction term is significant
m_int <- lm(score_posttest_c ~ group_e * score_pretest_c, data = df)

# extract coefficients - interaction term
tmp <- car::Anova(m_int, type = "III")
f_val <- tmp$`F value`[4]
p_val <- tmp$`Pr(>F)`[4]
numdf <- tmp$Df[4]
dendf <- tmp$Df[5]

# add to markdown
if (p_val > 0.05) {
  cat("The lack of an interaction between the independent variable and covariate was further confirmed by an non-significant interaction term in the model, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
} else {
  cat("The interaction between the independent variable and covariate was significant, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
}

# grouped scatterplot with 
plt <- ggplot(data = df, aes(x = score_pretest_c, y = score_posttest_c)) +
  geom_point(color = nondominant_col) +
  facet_wrap(. ~ group, nrow = 2) + 
  stat_smooth(formula = y ~ x, method = "lm", fullrange = T, se = T, alpha=0.2, color = dominant_col) +
  theme + 
  #'scale_x_continuous(breaks=seq(0, 10, 2)) +
  xlab("Prior knowledge assessment") + ylab("Learning outcome assessment") +
  stat_regline_equation(label.y = 1.5, aes(label = ..eq.label..)) +
  stat_regline_equation(label.y = 1, aes(label = ..rr.label..))
plt


# plt <- ggplot(data = df, aes(x = score_pretest, y = score_posttest)) +
#   geom_point(aes(color = group)) +
#   #facet_wrap(. ~ group, nrow = 2) + 
#   stat_smooth(formula = y ~ x, method = "lm", fullrange = T, aes(color = group, fill = group), se = T, alpha=0.2) +
#   theme + #theme(legend.position = "top") +
#   scale_color_manual(values = c(teal, orange)) + scale_fill_manual(values = c(teal, orange)) +
#   xlab("Prior knowledge assessment") + ylab("Learning outcome assessment") + labs(col = "Introductory material", fill = "Introductory material") +
#   
#   scale_x_continuous(breaks=seq(0, 10, 2))
# plt




```

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# Assumptions of homogeneity of variances

# add residuals to df
df$res_m2 <- m2$residuals

# test homogeneity of variances
tmp <- bartlett.test(res_m2 ~ group, data = df)

# extract values
p_val <- tmp$p.value
stat <- tmp$statistic

# add to markdown
if (p_val > 0.05) {
  cat("The results of the Bartlett test indicated that, following the removal of outliers, the assumption of homogeneity of the residual variances was met, Bartlett's K-squared = ", round(stat, 3), ", p = ", round(p_val, 3), ".", sep = "")
} else {
  cat("The results of the Bartlett test indicated that, following the removal of outliers, the assumption of homogeneity of the residual variances was not met, Bartlett's K-squared = ", round(stat, 3), ", p = ", round(p_val, 3), ".", sep = "")
}


```

> Please note that more participants may need to be excluded from the analyses as engagement data becomes available.

##### ANCOVA

An ANCOVA was run to determine the effect of introductory material on the learning outcome assessment score after controlling for prior knowledge assessment scores.

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# generate Ancova output
tmp <- car::Anova(m2, type = "III")

# extract coefficients - group effect
f_val <- tmp$`F value`[2]
p_val <- tmp$`Pr(>F)`[2]
numdf <- tmp$Df[2]
dendf <- tmp$Df[4]

# add to markdown - group effect
if (p_val > 0.05) {
  cat("After adjustment for prior knowledge assessment score, there was no statistically significant difference in learning outcome assessment score between the groups, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
} else {
  cat("After adjustment for prior knowledge assessment score, there was a statistically significant difference in learning outcome assessment score between the groups, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
}


# extract coefficients - covariate
f_val <- tmp$`F value`[3]
p_val <- tmp$`Pr(>F)`[3]
numdf <- tmp$Df[3]
dendf <- tmp$Df[4]

# add to markdown - covariate
if (p_val > 0.05) {
  cat("After adjustment for prior knowledge assessment score, there was no statistically significant difference in learning outcome assessment score between the groups, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
} else {
  cat("After adjustment for prior knowledge assessment score, there was a statistically significant difference in learning outcome assessment score between the groups, F(", numdf, ", ", dendf, ") = ", round(f_val, 2), ", p = ", round(p_val, 3), ".", sep = "")
}
```

The results are summarised as type-III analysis-of-variance table below.
```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# lsr::etaSquared(m2) computes [partial] eta square
out <- merge(tmp, lsr::etaSquared(m2), by = 0, all.x = T)

out <- out[!out$Row.names == "(Intercept)", ] # remove intercept
out$Row.names <- c("AO effect", "Residuals", "Prior knowledge") # rename
out <- out[order(out$Row.names),]

print(knitr::kable(out))
cat('\n\n<!-- -->\n\n')


cat('\n\nAdditionally, the results were transformed into the OLS regression output.\n\n')
sjPlot::tab_model(m2, show.stat = T, show.fstat = T, show.df = T, pred.labels = c("Intercept", "AO effect", "Prior knowledge"))
cat('\n\n<!-- -->\n\n')



```

This indicates that our primary hypothesis - that exposure to an AO (compared to a control condition) during teacher professional development ahead of new, to-be-learned material will facilitate learning of the new material - could not be confirmed. The lack of effect of introductory material is further illustrated in the figure below, plotting the mean of the learning outcome assessment score in each group, adjusted for the prior knowledge effect.  

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# define model with raw data for plotting
m <- lm(score_posttest ~ group + score_pretest, data = df)

# extract adjusted means and SE into df for plotting
adjusted_means <- effects::effect("group", m, se = T)

tmp <- data.frame(group = adjusted_means$variables$group$levels)
tmp$emm <- adjusted_means$fit
tmp$se <- adjusted_means$se


plt <- ggplot(data = tmp, aes(x = group, y = emm)) +
  geom_point(colour = dominant_col) +
  geom_errorbar(aes(ymin = emm - se, ymax = emm + se), width = .05, col = dominant_col) +
  coord_cartesian(ylim = c(min(df$score_posttest), max(df$score_posttest))) +
  xlab("Introductory material") + ylab("Adjusted mean learning outcome assessment") +
  labs(caption = "Error bars represent SE.") +
  theme +
  theme(plot.caption = element_text(hjust=0))
plt

```


### Exploratory analyses

##### Intervals

> These analyses cannot be completed yet as engagement data is required. 

##### Demographics

for the subset of participants that voluntarily provided demographic information, gender, and experience as classroom teacher were be included as additional covariates to test whether including additional demographic variables impacts the results. To also understand whether the manipulation may have differential effects depending on demographic variables, interaction terms with the effect-coded group variable were included. 


```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
cat("Age was removed as predictor due to being highly correlated with experience, r = ", round(cor(df$age, df$experience, use = "pairwise.complete.obs"), 2),".", sep = "")
```

However, as shown in the regression table below, demographic variables did not predict performance in the learning outcome assessment and also did not interact with the introductory material manipulation.

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# include demographics as covariates and interaction terms
m4 <- lm(score_posttest_c ~ group_e + score_pretest_c + gender + experience + gender:group_e + experience:group_e, data = df)

# add to markdown
sjPlot::tab_model(m4, show.stat = T, show.fstat = T, show.df = T)

```


##### Theta estimates from 2-paramter item-response-theory (IRT) models  

To create an index of prior knowledge, it is possible to simply add up items to a sum score as done above. However, measurement theories and more specifically item response theory (IRT) can be applied to measure the latent knowledge score.  

A uni-dimensional model was specified and the data for each item were supplied to the model using the *'mirt package'* (Chalmers, 2012) to separately fit a two or three parameter logistic model, respectively. However, both models failed to converge. Exploratory analysis using theta estimates were hence not further pursued.

```{r, echo = F}
# source: https://philippmasur.de/2022/05/13/how-to-run-irt-analyses-in-r/

# r code for IRT
unimodel <- 'F1 = 1-10'

# 2 parameter model
fit2PL <- mirt(data =  df[, paste0("question_", c(1:10), "_score")], 
               model = unimodel,  # alternatively, we could also just specify model = 1 in this case
               itemtype = "2PL", 
               verbose = FALSE)
fit2PL

# 3 parameter model
fit3PL <- mirt(data =  df[, paste0("question_", c(1:10), "_score")], 
               model = unimodel,  # alternatively, we could also just specify model = 1 in this case
               itemtype = "3PL", 
               verbose = FALSE)
fit3PL


```
```{r, include = F, results='asis', fig.align='center', warning=FALSE}
# model comparison
tmp <- anova(fit2PL, fit3PL)

# add to markdown
cat("A 2PL and a 3PL model were fitted. However, similar as shown using pilot data, the 3PL model did not fit the data better, X2(", tmp$df[2], ") = ", round(tmp$X2[2], 2), ", p = ", round(tmp$p[2], 3), ". ", sep = "")

cat("Hence, the simpler 2PL model was preferred.")

# add to markdowm
print(knitr::kable(tmp))
cat('\n\n<!-- -->\n\n')

```

### Learning experience survey

Upon completion of the learning assessment, participants were be asked to complete a short learning experience questionnaire (Wang et al., 2021).

> The type of learning material presented did not affect the learning experience ratings.

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}

# define learning experience items
items <- c(
  "Overall, I was satisfied with the introductory material presented   ahead of the evidence summary in module 1 of course 3 ('Learning to read').",
  "Overall, I was satisfied with the evidence summary presented  in  module 1 of course 3 ('Learning to read').",
  "How much mental effort did you invest to learn the content from the introductory material   and evidence summary presented in module 1 of course 3 ('Learning to read')?",
  "How difficult was it for you to learn the content from the introductory material and evidence summary presented in module 1 of course 3 ('Learning to read')?"
)

anchors_low <- c(
  "Strongly disagree",
  "Strongly disagree",
  "Very, very low mental effort",
  "Very, very easy"
)

anchors_high <- c(
  "Strongly agree",
  "Strongly agree",
  "Very, very high mental effort",
  "Very, very difficult"
)


# define vars
vars <- c("sat_im_score", "sat_lm_score", "effort_score", "difficulty_score")
labels <- c("Satisfaction with introductory material",
            "Satisfaction with learning material",
            "Effort",
            "Difficulty")
grp <- "group"



for (v in 1:length(vars)) {
  
  var <- vars[v]
  
  # add header
  cat("#####", labels[v], "\n\n")
  
  cat("To assess ", tolower(labels[v]), ", participants were asked to rate the item '", items[v], "' on a 7-point Likert scale from 1 = '", anchors_low[v], "' to 7 = '", anchors_high[v], "'.\n\n", sep = "")
  
  
  # generate and print descriptive tables to markdown
  table_desc(data = df, group_var = grp, dep_var = var)    
  
  # create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
  plt <- plot_raincloud(data = df, xvar = grp, yvar = var, 
                        xlower = 1, xupper = 2,
                        ylower = 1, yupper = 7, ybreaks = 1,
                        #title = "Descriptive visualisation of rating in each group",
                        xlab = "Introductory material",
                        ylab = "Rating", 
                        note = "Error bars represent SE.")
  
  # run two sample t-test
  tmp <- t.test(df[, var] ~ df$group)
  
  # extract values
  t_val <- tmp$statistic
  p_val <- tmp$p.value
  df_val <- tmp$parameter
  
  
  # add to markdown
  if (p_val > 0.05) {
    cat("No significant difference in ", tolower(labels[v]), " was found between the groups, t(", round(df_val, 2), ") = ", round(t_val, 2), ", p = ", round(p_val, 3), ".\n\n", sep = "")
  } else {
    cat("A significant difference in ", tolower(labels[v]), " was found between the groups, t(", round(df_val, 2), ") = ", round(t_val, 2), ", p = ", round(p_val, 3), ".\n\n", sep = "")
  }
  
  
}

```