---
title: "Piloting the prior knowledge assessment"
author: "Stefanie Meliss"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
rm(list = ls())
library(ggplot2)
library(GGally)
library(mirt)
library(ggmirt)

# read in responses (choice text extracted)
df <- read.csv("data_export_jan24_txt.csv", stringsAsFactors = F)

# remove first two rows
df <- df[grepl("2023", df$StartDate),]

# only select complete observations
df <- df[grepl("True", df$Finished),]

# read in questionnaire files
quest <- read.csv("pretest_pilot.csv", stringsAsFactors = F)
quest$order <- factor(quest$order, levels = paste0("Q",1:nrow(quest)))
rand <- read.csv("pretest_pilot_rand.csv", stringsAsFactors = F)
quest <- merge(quest, rand, by = c("question", "order"))

# create long format of rand quest (for later merging)
rand <- reshape2::melt(rand, id = c("question", "order"))

# define function for plotting
source("functions.R")

# average duration
mean(as.numeric(df$`Duration..in.seconds.`))/60
```

### Description of the pilot study

The purpose of this pilot study was to administer the prior knowledge assessment to teachers to determine whether the items possess the right level of difficulty. The prior knowledge assessment was designed for the use in this project (by CS & SA). In addition to an empirical variance measure for each of the ten prior knowledge questions, we also collected feedback on the assessment.  

##### Participants  

Participants were recruited using Twitter. The pilot study was set up in Qualtrics and a distribution link was shared (by SS). In total, the survey was completed by **135** participants. The project protocol was approved by University College London's ethics committee. Participants were informed about the purpose and provided informed consent to participate in the pilot study.   

Participants were recruited using Twitter. The pilot study was set up in Qualtrics and a distribution link was shared (by SS). In total, the survey was started by 135 participants and completed by **111** participants. The project protocol was approved by University College London's ethics committee. Participants were informed about the purpose and provided informed consent to participate in the pilot study.   

##### Materials and measurements  

The prior knowledge assessment consisted of ten four-alternative forced choice questions/statements. For each presented item (e.g., "A grapheme is an individual letter or a group of letters that representsâ€¦" ), participants were instructed to select the answer out of four alternatives they thought as correct (e.g., "A syllable.", "A phoneme.", "A word.", "A morpheme."). Participants were encouraged to guess but specifically asked to not look up any correct answers. The items and corresponding options were presented in randomised, but constant order, i.e., a custom-made R script was used to randomise the presentation order and the same order was used across all participants.  
The feedback survey consisted of in total six items. We asked participants to rate the amount and difficulty of questions (bipolar 5-point Likert scale from -2 = "too few"/"too easy" to +2 = "too many"/"too difficult"). Participants were additionally instructed to choose the midpoint to indicate that the questions were neither too few/too easy nor too many/too difficult. Lastly, a 5-point Likert scale (1 = "strongly disagree", 5 = "strongly agree") was presented to participants to obtain ratings regarding the wording of the questions (e.g., "The questions in the multiple choice test are worded in a way that is concise.") For each ratings, an optional free-text input text box was added, offering participants to opportunity to expand on their rating.  

##### Procedure  

Upon following the Qualtrics survey distribution link, participants were informed about the purpose of the study and asked to provide consent. No identifiable information was recorded but participants were asked to create a study pseudonym should they wish to withdraw their consent. Participants then completed the prior knowledge assessment (presented on a single page) followed by the feedback survey (presented on one page). Participants took on average **5.02** min to complete the pilot study.  

Upon following the Qualtrics survey distribution link, participants were informed about the purpose of the study and asked to provide consent. No identifiable information was recorded but participants were asked to create a study pseudonym should they wish to withdraw their consent. Participants then completed the prior knowledge assessment (presented on a single page) followed by the feedback survey (presented on one page). Participants took on average **5.81** min to complete the pilot study.  


### Performance in the prior knowledge assessment

```{r, include=F}
# extract ordered question index
qs <-sort(quest$order)

binom <- data.frame(q = character(length(qs)),
                    n_success = numeric(length(qs)),
                    n_trial = numeric(length(qs)),
                    p_val = numeric(length(qs)),
                    ci_lower = numeric(length(qs)),
                    ci_upper = numeric(length(qs)))


# --- loop through all questions to compute score, assign label, etc --- #
for (q in 1:length(qs)) {
  
  # determine correct answer string
  correct <- quest$correct[quest$order == as.character(qs[q])]
  
  # create response and score variable for each question
  df[,paste0(qs[q], "_response")] <- df[,as.character(qs[q])]
  df[,paste0(qs[q], "_score")] <- ifelse(df[,as.character(qs[q])] == correct, 1, 0)
  
  # extract information from rand: labels each answer option according to its order of occurrence and contains question text
  tmp <- subset(rand, order == as.character(qs[q]))
  tmp$order <- NULL
  names(tmp) <- c(paste0(qs[q], "_question"), paste0(qs[q], "_label"), paste0(qs[q], "_response"))
  
  # select subset of columns of df --> allows later merging to be "prettier" with respect to the order of columns
  tmp2 <-  df[,c("ResponseId", paste0(qs[q], "_response"))] 
  
  # merge tmp objects
  tmp2 <- merge(tmp2, tmp, by = paste0(qs[q], "_response"))
  tmp2[,paste0(qs[q], "_response")] <- NULL
  df <- merge(df, tmp2, by = "ResponseId")
  
  # remove tmp objects
  rm(tmp, tmp2)
  
  # compute binomial test
  test <- binom.test(x = sum(df[, paste0(qs[q], "_score")]),
                     n = nrow(df), 
                     p = nrow(quest)/4/10,
                     alternative = "greater")
  # populate table
  binom$q[q] <- as.character(qs[q])
  binom$n_success[q] <- test$statistic
  binom$n_trial[q] <- test$parameter
  binom$p_val[q] <- test$p.value
  binom$ci_lower[q] <- test$conf.int[1]
  binom$ci_upper[q] <- test$conf.int[2]
  
  
  # delete tmp column
  df[,as.character(qs[q])] <- NULL
  
}
```


To determine how participants performed in the prior knowledge assessment, the selected response was compared to the correct response using custom-made R scripts and points were awarded if the correct response was selected. The total score was calculated as the sum of points obtained in all 10 items.

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}
# complete total performance score
df$q_total <- rowSums(df[,c(grepl("_score", names(df)))])

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(df, "Finished", "q_total", 
               xlower = 1, xupper = 1,
               ylower = 0, yupper = nrow(quest), ybreaks = 2, yintercept = nrow(quest)/4,
               title = "Descriptive visualisation of performance in prior knowledge assessment",
               ylab = "Total number of correct items") + theme(axis.title.x = element_blank(), axis.text.x = element_blank()) 
print(suppressWarnings(plt))

```

Above, a "raincloud" plot was used to visualise the data. A raincloud plot used here combines a half violin plot (illustrating the distribution of the data, i.e., the charcole-coloured cloud) with a boxplot (showing summary statistics, i.e., rectangle with whiskers) and a dot plot (capturing the raw data, i.e., the charcole-coloured rain). Additionally, the mean (+/- standard error) were added in coral together with a horizontal line marking chance level performance.  

Descriptive statistics can further be found in the table below.

```{r, echo = F, results='asis', fig.align='center'}
tmp <- psych::describe(df$q_total)
row.names(tmp) <- NULL
tmp$vars <- NULL
print(knitr::kable(tmp))
cat('\n\n<!-- -->\n\n')
```

After determining whether the total score was normally distributed, the appropriate statistical test (i.e., one-Sample t-test or the non-parametric Wilcoxon signed rank test) was used to determine whether the performance was significantly better than chance (i.e., randomly picking the correct out of 4 alternatives, 1/4 = 25%).

```{r, echo=F}
# test for normal distribution
if (shapiro.test(df$q_total)$p < 0.05){
  # use non-parametric test if data is not normally distributed
  wilcox.test(df$q_total, mu = nrow(quest)/4, alternative = "greater")
} else {
  # use parametric t-test
  t.test(df$q_total, mu = nrow(quest)/4, alternative = "greater")
}

```

In addition to the total test score, each item was examined separately. For this, the proportion of correct responses was plotted for each item below. The dotted horizontal line represents chance-level performance. The error bars represent the 95% confidence intervals estimated based upon binomial tests, testing the null hypothesis that the number of correct responses is greater than chance level.

```{r, echo = F, results='asis', fig.align='center', warning=FALSE}

# --- visualise average performance on each item --- #

# compute proportion of correct responses
binom$perc <- binom$n_success/binom$n_trial
binom$q <- factor(binom$q, levels = paste0("Q", 1:nrow(quest)))

# use desc as input for a bar graph for each question
ggplot(binom, aes(x = q, y = perc)) +
  geom_bar(stat = "identity", fill = ambition_coral) +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.2) +
  geom_hline(yintercept = 0.25, linetype="dashed") +
  theme_bw() + 
  coord_cartesian(ylim = c(0,1)) + scale_y_continuous(labels = scales::label_percent(), breaks=seq(0,1,0.1)) +
  ylab("Proportion correct") +
  xlab("Questions") +
  ggtitle("Comparison to chance-level peformance")
```

Further, answer patterns in each question were investigated by plotting the proportion each response option was selected with. The corresponding item text and answer options were also included. The correct answer was highlighted in coral.

```{r, results='asis', fig.align = "center", echo = F}
# extract ordered question index
qs <-sort(quest$order)

# loop through all questions
for (q in 1:length(qs)) {
  
  # determine correct answer string
  correct <- quest$correct[quest$order == as.character(qs[q])]
  #correct <- gsub("[*]","", correct)
  #correct <- gsub("[*]","", correct)
  
  # print to console/markdown
  cat("  \n##### ", as.character(qs[q]))
  cat("  \n")
  cat("  \n**Question: ", quest$question[quest$order == as.character(qs[q])],"**", "*[ correct: ", correct, "]*")
  cat("  \n\n Option 1: ", quest$option1[quest$order == as.character(qs[q])])
  cat("  \n Option 2: ", quest$option2[quest$order == as.character(qs[q])])
  cat("  \n Option 3: ", quest$option3[quest$order == as.character(qs[q])])
  cat("  \n Option 4: ", quest$option4[quest$order == as.character(qs[q])])
  
  # create this tmp column where the score (i.e., 0 or 1) is transformed into a character variable
  # will be used to determine fill colour in bar graphs
  df[,as.character(qs[q])] <- as.character(df[, paste0(qs[q], "_score")])
  df[,paste0(qs[q], "_label")] <- gsub("n", "n ", df[,paste0(qs[q], "_label")]) # add space
  df[,paste0(qs[q], "_label")] <- stringr::str_to_title(df[,paste0(qs[q], "_label")]) # add space
  
  
  plt <- ggplot(df, aes(x = get(paste0(qs[q], "_label")), fill = get( as.character(qs[q]) ))) +
    geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) +
    theme_bw() +
    xlab("Answer options") +
    ylab("Proportion selected") + coord_cartesian(ylim = c(0,1)) +
    #ggtitle(unique(df[, paste0(qs[q], "_question")])) +
    scale_fill_manual(values = c(ambition_charcole, ambition_coral)) +
    theme(legend.position = "none") +
    #scale_x_discrete(labels = paste("Option", 1:4)) +
    scale_y_continuous(labels = scales::label_percent(), breaks=seq(0,1,0.1))
  
  print(plt)
  
  # delete tmp column
  df[,as.character(qs[q])] <- NULL
  
  cat("  \n")
  
}
```

### Feedback survey  

Feedback regarding the prior knowledge test was evaluated.  

```{r, include= F}
# rename variables that contain feedbak info
names(df)[names(df) == "quest_amount_1"] <- "quest_amount"
names(df)[names(df) == "quest_difficulty_1"] <- "quest_difficulty"
names(df)[names(df) == "quest_rating_1"] <- "quest_rat_concise"
names(df)[names(df) == "quest_rating_2"] <- "quest_rat_easy_to_understand"
names(df)[names(df) == "quest_rating_3"] <- "quest_rat_comprehensive"
names(df)[names(df) == "quest_rating_4"] <- "quest_rat_unambiguous"
```

##### Amount of questions

The raincloud plot below illustrate how participants have rated the item.  
```{r, echo=FALSE, results='asis', fig.align='center'}
# declare factor
df$quest_amount <- factor(df$quest_amount, levels = c("too few", "few", "neither nor", "many", "too many"))
# assign numeric value
df$quest_amount_score <- ifelse(df$quest_amount == "too few", -2,
                                ifelse(df$quest_amount == "few", -1,
                                       ifelse(df$quest_amount == "neither nor", 0,
                                              ifelse(df$quest_amount == "many", 1,
                                                     ifelse(df$quest_amount == "too many", 2,
                                                            NA)))))

# # create bargraph
# plt <- ggplot(df, aes(x = quest_amount)) +
#   geom_bar(aes(y = (after_stat(count))/sum(after_stat(count))), fill = ambition_charcole) +
#   theme_bw() +
#   coord_cartesian(ylim = c(0,1)) + scale_y_continuous(labels = scales::label_percent()) +
#   scale_x_discrete(breaks = c("too few", "", "neither nor", "", "too many")) +
#   xlab("Answer options") +
#   ylab("Proportion") 
# print(plt)

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(df, "Finished", "quest_amount_score", 
               xlower = 1, xupper = 1,
               ylower = -2, yupper = 2, 
               ylab = "Rating")  + theme(axis.title.x = element_blank(), axis.text.x = element_blank()) 
print(suppressWarnings(plt))

```

Descriptive statistics associated with the ratings are shown below. Please note that "too few" was coded as -2 whereas "too many" was coded as +2. An average rating of/close to zero would represent that the amount of question was neither too few nor too many.

```{r, echo=FALSE, results='asis', fig.align='center'}
# calculate descriptives in table
tmp <- psych::describe(df$quest_amount_score)

row.names(tmp) <- NULL
tmp$vars <- NULL

print(knitr::kable(tmp))
cat('\n\n<!-- -->\n\n')

```

Whether the observed rating was significantly different from zero was determined using (non-)parametric tests. 

```{r, echo=F}
# test for normal distribution
if (shapiro.test(df$quest_amount_score)$p < 0.05){
  # use non-parametric test if data is not normally distributed
  wilcox.test(df$quest_amount_score)
} else {
  # use parametric t-test
  t.test(df$quest_amount_score)
}

```

Participants were also provided with the option to expand on the rating using comments.

```{r, echo=F}
for (i in 1:length(unique(df$quest_amount_com))) {
  cat(unique(df$quest_amount_com)[i], "\n\n")
}

```


##### Difficulty of questions

Similar to the amount of questions, the difficulty of questions was rated using a bipolar Likert scale (-2 = "too easy", +2 = "too difficult"). The ratings provided by participants are plotted below. 
```{r, echo=FALSE, results='asis', fig.align='center'}

# declare factor
df$quest_difficulty <- factor(df$quest_difficulty, levels = c("too easy", "easy", "neither nor", "difficult", "too difficult"))
# assign numeric value 
df$quest_difficulty_score <- ifelse(df$quest_difficulty == "too easy", -2,
                                    ifelse(df$quest_difficulty == "easy", -1,
                                           ifelse(df$quest_difficulty == "neither nor", 0,
                                                  ifelse(df$quest_difficulty == "difficult", 1,
                                                         ifelse(df$quest_difficulty == "too difficult", 2,
                                                                NA)))))

# # create bar graph
# plt <- ggplot(df, aes(x = quest_difficulty)) +
#   geom_bar(aes(y = (after_stat(count))/sum(after_stat(count))), fill = ambition_charcole) +
#   theme_bw() +
#   coord_cartesian(ylim = c(0,1)) + scale_y_continuous(labels = scales::label_percent()) +
#   scale_x_discrete(breaks = c("too easy", "", "neither nor", "", "too difficult")) +
#   xlab("Answer options") +
#   ylab("Proportion") 
# print(plt)

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(df, "Finished", "quest_difficulty_score", 
               xlower = 1, xupper = 1,
               ylower = -2, yupper = 2, 
               ylab = "Rating") + theme(axis.title.x = element_blank(), axis.text.x = element_blank()) 
print(suppressWarnings(plt))

```

Descriptive summary statistics of the difficulty rating were computed and statistically compared against zero.  

```{r, echo=FALSE, results='asis', fig.align='center'}
# calculate descriptives in table
tmp <- psych::describe(df$quest_difficulty_score)

row.names(tmp) <- NULL
tmp$vars <- NULL

print(knitr::kable(tmp))
cat('\n\n<!-- -->\n\n')

```


```{r, echo=F}
# test for normal distribution
if (shapiro.test(df$quest_difficulty_score)$p < 0.05){
  # use non-parametric test if data is not normally distributed
  wilcox.test(df$quest_difficulty_score)
} else {
  # use parametric t-test
  t.test(df$quest_difficulty_score)
}

```
Participants were also provided with the option to expand on the rating using comments.

```{r, echo=F}
for (i in 1:length(unique(df$quest_difficulty_com))) {
  cat(unique(df$quest_difficulty_com)[i], "\n\n")
}
```


#### Other ratings

Lastly, a 5-point Likert scale rating matrix was presented to participants to obtain further ratings on the questions. Each item was rated on a scale from -2 ("strongly disagree") to 2 ("strongly agree"). Raincloud plots were used to visualise distribution and summary statistics for each rating.

```{r, include=FALSE, results='asis', fig.align='center'}
vs <- names(df)[grepl("_rat_", names(df)) ]
for (v in 1:length(vs)) {
  
  # declare factor
  df[,vs[v]] <- factor(df[,vs[v]], levels = c("Strongly disagree", "Somewhat disagree", "Neither agree nor disagree", "Somewhat agree", "Strongly agree"))
  
  # assign numeric value
  df[,paste0(vs[v], "_score")] <- ifelse(df[,vs[v]] == "Strongly disagree", -2,
                                         ifelse(df[,vs[v]] == "Somewhat disagree", -1,
                                                ifelse(df[,vs[v]] == "Neither agree nor disagree", 0,
                                                       ifelse(df[,vs[v]] == "Somewhat agree", 1,
                                                              ifelse(df[,vs[v]] == "Strongly agree", 2,
                                                                     NA)))))
  # rescale so that variable 
  #df[,paste0(vs[v], "_score")] <- df[,paste0(vs[v], "_score")] + 3
  
}


# # plot data as bargraph
# plt <- ggplot(desc, aes(x = vars, y = mean)) +
#   geom_point(stat = "identity", fill = ambition_charcole) +
#   geom_errorbar(aes(ymin=mean-1.96*se, ymax=mean+1.96*se), width=.1) +
#   #geom_hline(yintercept = 0.25, linetype="dashed") +
#   theme_bw() + coord_cartesian(ylim = c(-2,2)) +
#   ylab("Average Rating") +
#   xlab("Items")
# print(plt)
```


```{r, echo=FALSE, results='asis', fig.align='center', fig.width=15}


# select subset of variables
desc <- df[,c(grepl("ResponseId", names(df)) | grepl("_rat_", names(df)))]
desc <- desc[,c(grepl("ResponseId", names(desc)) | grepl("score", names(desc)))]

# tranform them into long format
desc <- reshape2::melt(desc, id.vars = "ResponseId")

desc$variable <- gsub("quest_rat_", "", desc$variable)
desc$variable <- gsub("_score", "", desc$variable)
desc$variable <- gsub("_", " ", desc$variable)

vars <- unique(desc$variable)
desc$variable <- factor(desc$variable, levels = vars)

# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(desc, "variable", "value", ylower = -2, yupper = 2, 
                      ylab = "Rating", xlab = "Items")
print(suppressWarnings(plt))
```
Additional descriptive statistics were computed and included below.

```{r, echo=FALSE, results='asis', fig.align='center'}
# select subset of variables
desc <- df[,c(grepl("ResponseId", names(df)) | grepl("_rat_", names(df)))]
desc <- desc[,c(grepl("ResponseId", names(desc)) | grepl("score", names(desc)))]

# tranform them into long format
desc <- reshape2::melt(desc, id.vars = "ResponseId")

# compute descriptive stats, seperately for each rating
desc <-psych::describeBy(desc$value, desc$variable)

# tranform list to df
desc <- do.call(rbind.data.frame, desc)

# create better identifier
desc$vars <- gsub("quest_rat_", "", row.names(desc))
desc$vars <- gsub("_score", "", desc$vars)
desc$vars <- gsub("_", " ", desc$vars)
row.names(desc) <- NULL

vars <- desc$vars
desc$vars <- factor(desc$vars, levels = vars)


# show descriptive stats
print(knitr::kable(desc))
cat('\n\n<!-- -->\n\n')

```

Participants were also provided with the option to expand on the rating using comments.

```{r, echo=F}
for (i in 1:length(unique(df$quest_rating_com))) {
  cat(unique(df$quest_rating_com)[i], "\n\n")
}
```


##### Intercorrelation matrix of all ratings

```{r, echo=FALSE, results='asis', fig.align='center', fig.height=10, fig.width=10, message=F, warning=F}
desc <- df[,c(grepl("ResponseId", names(df)) | grepl("quest_", names(df)))]
desc <- desc[,c(grepl("ResponseId", names(desc)) | grepl("score", names(desc)))]
desc$correct_responses <- df$q_total

names(desc) <- gsub("quest_", "", names(desc))
names(desc) <- gsub("rat_", "", names(desc))
names(desc) <- gsub("_score", "", names(desc))
names(desc) <- gsub("_", "", names(desc))

lowerfun <- function(data,mapping){
  ggplot(data = data, mapping = mapping)+
    geom_point(position=position_jitter(height=0.25, width=0.25), col = ambition_charcole) +
  coord_cartesian(ylim = c(-2,2), xlim = c(-2,2))

} 

diagfun <- function(data,mapping){
  
  ggplot(data = data, mapping = mapping)+
    geom_histogram(stat = "count", binwidth = 1, fill = ambition_coral) + 
    coord_cartesian(ylim = c(0,100), xlim = c(-2,2))
} 

plt <- ggpairs(desc[, -1], lower = list(continuous = wrap(lowerfun)), diag = list(continuous =  wrap(diagfun))) + theme_bw() 

for (i in c(1:6)) {
  plt[7,i] <- plt[7,i] + coord_cartesian(ylim = c(0,10), default = T) + scale_y_continuous(breaks=seq(0,10,2))
}

plt[7,7] <- plt[7,7] + coord_cartesian(xlim = c(0,10), default = T) + scale_x_continuous(breaks=seq(0,10,2))

print(suppressWarnings(plt))
```

##### Item Response Theory (IRT)

To create an index of prior knowledge, it is possible to simply add up items to a sum score as done above. However, measurement theories and more specifically item response theory (IRT) can be applied to measure the latent knowledge score. IRT models have three parameters:  
  
* *discrimination* (i.e., ability of an item to differentiate between respondents with different levels of proficiency)  
* *difficulty* (i.e., likelihood of a correct response, expressed as the proficiency level at which 50% of the participant sample is estimated to answer an item correctly)  
* *guessing probability*  

As a first step, a uni-dimensional model is specified and the data for each item is supplied to the model before fitting the model using the *'mirt package'* (Chalmers, 2012). A 2PL and a 3PL model were fitted, however, as shown below, the 3PL model did not fit the data and hence, the simpler 2PL model was preferred. 

```{r, echo=F}
# source: https://philippmasur.de/2022/05/13/how-to-run-irt-analyses-in-r/

# r code for IRT
unimodel <- 'F1 = 1-10'

# 2 parameter model
fit2PL <- mirt(data =  df[, paste0("Q", c(1:10), "_score")], 
               model = unimodel,  # alternatively, we could also just specify model = 1 in this case
               itemtype = "2PL", 
               verbose = FALSE)
fit2PL

# 3 parameter model
fit3PL <- mirt(data =  df[, paste0("Q", c(1:10), "_score")], 
               model = unimodel,  # alternatively, we could also just specify model = 1 in this case
               itemtype = "3PL", 
               verbose = FALSE)
fit3PL

# model comparison
anova(fit2PL, fit3PL)
```
To understand the IRT parameters, the so-called factor solution including factor loadings (F1) and the communalities (h2), which are squared factor loadings and are interpreted as the variance accounted for in an item by the latent trait. Substantive relationship with the latent trait are defined as loadings > .50.

```{r, echo = F}
# Factor solution
summary(fit2PL)
```

The 2PL model takes item discrimination (first parameter: a) and item difficulty (second parameter: b) into account while guessing probability (third parameter: c or g) is held constant.  
  
Parameter a represents the values of the slope and the values fairly evenly distributed in the 2PL model, yet larger values, i.e., steeper slopes are better at differentiating people as higher slope value indicate stronger relationships between item and latent trait.  
  
The location or difficulty parameters (parameter b) is also listed for each item. Location parameters are interpreted as the value of theta that corresponds to a .50 probability of responding correctly at or above that location on an item. The location parameters show that the items cover a wide range of the latent trait in the negative, but not the positive direction.  

```{r, echo = FALSE}
params2PL <- coef(fit2PL, IRTpars = TRUE, simplify = TRUE)
round(params2PL$items, 2) # g = c = guessing parameter
```
Below, the model fit is quantified using *M2*, a statistic designed to assess the fit of IRT models. The IRT model results in a non-significant M2 statistic, indicating that there is no significant difference between the model and the data.

```{r, echo = FALSE}
M2(fit2PL)
```

Item characteristic curves visualise the IRT parameters for each item to help understand the properties of each item. As illustrated below and in alignment with what has been said above, the items predominantly differentiate among the negative values of theta and particularly close to zero.  

```{r, echo = F}
ggmirt::tracePlot(fit2PL, theta_range = c(-20, 20), facet = F, legend = T) + scale_color_brewer(palette = "Paired")
```
Information curves (plotted below) refer to the ability of an item to estimate theta scores. 

```{r}
ggmirt::itemInfoPlot(fit2PL, theta_range = c(-20, 20), legend = T) + scale_color_brewer(palette = "Paired")
```

IRT can also be used to estimate the latent score of each subject. The descriptives are included and plotted below. 

```{r, echo = F, results='asis'}
# extract ability estimates for each subject 
df$q_2pl <- fscores(fit2PL)
desc <- psych::describe(df$q_2pl)

# show descriptive stats
print(knitr::kable(desc))
cat('\n\n<!-- -->\n\n')
cat('\n')

```

```{r, echo=F}
# create rain cloud plot, adapted from: https://z3tt.github.io/Rainclouds/
plt <- plot_raincloud(df, "Finished", "q_2pl", 
               xlower = 1, xupper = 1,
               ylower = -2, yupper = 2, ybreaks = 0.5,
               title = "Descriptive visualisation of performance in prior knowledge assessment",
               ylab = "Theta") + theme(axis.title.x = element_blank(), axis.text.x = element_blank()) 
print(suppressWarnings(plt))
```

The latent score and sum scores are highly correlated with each other.  
```{r, echo = F}
ggplot(df, aes(x = q_total, y = q_2pl)) +
  geom_point(col = ambition_charcole) + 
  geom_smooth(method=lm, col = ambition_coral, fullrange = T) +
  theme_bw() +
  ggtitle("Relationship Between Sum Score And IRT Approach") +
  ylab("Theta")+xlab("Sum score") +
  coord_cartesian(xlim = c(0,nrow(quest)), ylim = c(-2,2)) + scale_x_continuous(breaks=seq(0,nrow(quest),2)) + scale_y_continuous(breaks=seq(-2,2,0.5)) +
  ggpubr::stat_cor(method = "pearson", cor.coef.name = "r", label.x = 2, label.y = 1)  # Add correlation coefficient
```

